{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4342d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from skimage.filters import laplace\n",
    "import numpy as np\n",
    "from scipy.fft import fft2,fftshift\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8118c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from joblib import Parallel,delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312cc759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c02eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    def __init__(self,image_dir,random_seed=69):\n",
    "        self.feature_names=['laplacian_var','laplacian_max','ma_laplacian','vlow_freqE','lowmid_freqE','mid_freqE','hog_mean','hog_std','hog_skewness','ninth_percentile']\n",
    "        \n",
    "        self.image_dir=image_dir\n",
    "        self.image_paths=[os.path.join(image_dir,fname) for fname in os.listdir(image_dir) if fname.endswith(('.jpg','.jpeg','.png')) ]\n",
    "        self.kernel_size_choices=(3,5,7)\n",
    "        self.target_image_size=(299,299)\n",
    "        self.metadata_df=self.__create_metadata_df(random_seed)\n",
    "        self.features_df=None\n",
    "        \n",
    "       \n",
    "    def __create_metadata_df(self,random_seed):\n",
    "        random.seed(random_seed)\n",
    "        all_sample_records=[]\n",
    "        for image_path in self.image_paths:\n",
    "            kernel_idx=(hash(image_path)+random.randint(0,1000))%len(self.kernel_size_choices)\n",
    "            kernel_size=self.kernel_size_choices[kernel_idx]\n",
    "            all_sample_records.append({\"image_path\":image_path,\"blur_type\":\"box\",\"kernel_size\":kernel_size})\n",
    "            all_sample_records.append({\"image_path\":image_path,\"blur_type\":\"gaussian\",\"kernel_size\":kernel_size})\n",
    "\n",
    "        return pd.DataFrame(all_sample_records)\n",
    "\n",
    "\n",
    "    def apply_blur(self,image_path,kernel_size,blur_type):\n",
    "        img=cv2.imread(image_path)\n",
    "        img = cv2.resize(img, (256, 256))\n",
    "        if blur_type=='box':\n",
    "            img=cv2.blur(img,(kernel_size,)*2)\n",
    "        else:\n",
    "            img=cv2.GaussianBlur(img,(kernel_size,)*2,sigmaX=0)\n",
    "\n",
    "        return img\n",
    "    \n",
    "\n",
    "    def extract_features(self,blurred_image):\n",
    "        \n",
    "        if len(blurred_image.shape)==3:\n",
    "            greyed_img=cv2.cvtColor(blurred_image,cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            greyed_img=blurred_image\n",
    "        \n",
    "        features=[]\n",
    "        \n",
    "        laplacian=laplace(greyed_img)\n",
    "        laplacian_variance=np.var(laplacian)\n",
    "        features.append(laplacian_variance)\n",
    "        features.extend([np.max(laplacian),np.mean(np.abs(laplacian))])\n",
    "        \n",
    "        fft_image=fftshift(np.abs(fft2(greyed_img)))\n",
    "        h,w=fft_image.shape\n",
    "        center_y,center_x=h//2,w//2\n",
    "\n",
    "        for radius in [h//8,h//6,h//4]:\n",
    "            mask=np.zeros((h,w))\n",
    "            cv2.circle(mask,(center_x,center_y),radius,1,-1)\n",
    "            features.append(np.mean(fft_image[mask==1]))\n",
    "\n",
    "        hog=cv2.HOGDescriptor((64,64),(16,16),(8,8),(8,8),9)\n",
    "        hog_features=hog.compute(greyed_img).flatten()\n",
    "        features.extend([np.mean(hog_features),np.std(hog_features),skew(hog_features),np.percentile(hog_features,90)])\n",
    "\n",
    "        return np.array(features)\n",
    "    \n",
    "\n",
    "    def generate_dataset(self, output_path=None, n_jobs=-1, batch_size=1000):\n",
    "\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        # Process in parallel batches\n",
    "        def process_batch(batch_indices):\n",
    "            batch_features = []\n",
    "            batch_labels = []\n",
    "            for idx in batch_indices:\n",
    "                row = self.metadata_df.iloc[idx]\n",
    "                blurred = self.apply_blur(\n",
    "                    row['image_path'],\n",
    "                    row['kernel_size'],\n",
    "                    row['blur_type']\n",
    "                )\n",
    "                feats = self.extract_features(blurred)\n",
    "                batch_features.append(feats)\n",
    "                batch_labels.append(0 if row['blur_type'] == 'box' else 1)\n",
    "            return batch_features, batch_labels\n",
    "        \n",
    "        # Create batch indices\n",
    "        indices = np.arange(len(self.metadata_df))\n",
    "        batches = [indices[i:i+batch_size] for i in range(0, len(indices), batch_size)]\n",
    "        \n",
    "        # Parallel processing\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            [delayed(process_batch)(batch) \n",
    "            for batch in tqdm(batches, desc=\"Processing images\")]\n",
    "        )\n",
    "        \n",
    "        # Combine results\n",
    "        for batch_feats, batch_labels in results:\n",
    "            features.extend(batch_feats)\n",
    "            labels.extend(batch_labels)\n",
    "        \n",
    "        # Create final DataFrame\n",
    "        self.features_df = pd.DataFrame(features, columns=self.feature_names)\n",
    "        self.features_df['image_path'] = self.metadata_df['image_path'].values\n",
    "        self.features_df['label'] = labels\n",
    "        \n",
    "        if output_path:\n",
    "            self.features_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        return self.features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793d0887-2ead-44c6-9843-41a14450e1ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:15:50.629292Z",
     "iopub.status.busy": "2025-06-14T16:15:50.628520Z",
     "iopub.status.idle": "2025-06-14T16:15:50.633622Z",
     "shell.execute_reply": "2025-06-14T16:15:50.632932Z",
     "shell.execute_reply.started": "2025-06-14T16:15:50.629269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b8492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score,classification_report,confusion_matrix,precision_recall_curve,roc_curve,auc\n",
    "import json\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b81ccb-d8a5-45ca-a5aa-80eda75ef401",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:08:59.788374Z",
     "iopub.status.busy": "2025-06-14T16:08:59.788122Z",
     "iopub.status.idle": "2025-06-14T16:08:59.803136Z",
     "shell.execute_reply": "2025-06-14T16:08:59.802426Z",
     "shell.execute_reply.started": "2025-06-14T16:08:59.788357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a1f12-1810-47cb-b35e-81d4017cd5cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:08:59.804711Z",
     "iopub.status.busy": "2025-06-14T16:08:59.804449Z",
     "iopub.status.idle": "2025-06-14T16:08:59.820100Z",
     "shell.execute_reply": "2025-06-14T16:08:59.819274Z",
     "shell.execute_reply.started": "2025-06-14T16:08:59.804690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('model',exist_ok=True)\n",
    "os.makedirs('results',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875cde4b-93f8-403a-9b43-3cf5ec3c5019",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:08:59.821163Z",
     "iopub.status.busy": "2025-06-14T16:08:59.820864Z",
     "iopub.status.idle": "2025-06-14T16:08:59.836626Z",
     "shell.execute_reply": "2025-06-14T16:08:59.836051Z",
     "shell.execute_reply.started": "2025-06-14T16:08:59.821141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BlurClassifierModel(nn.Module):\n",
    "    def __init__(self, input_features_no=10, dropout_rate=0.3):\n",
    "        super(BlurClassifierModel, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(input_features_no, 512)\n",
    "        self.input_bn = nn.BatchNorm1d(512)\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "        )\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input processing\n",
    "        x = self.gelu(self.input_bn(self.input_layer(x)))\n",
    "        \n",
    "        # Residual blocks\n",
    "        identity = x\n",
    "        x = self.block1(x)\n",
    "        x = self.gelu(x + identity)  # Residual connection\n",
    "        \n",
    "        x = self.block2(x)\n",
    "        x = self.gelu(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        x = self.gelu(x)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028cad39-1ca7-40cc-8d11-e423c192f025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:08:59.837401Z",
     "iopub.status.busy": "2025-06-14T16:08:59.837242Z",
     "iopub.status.idle": "2025-06-14T16:08:59.856111Z",
     "shell.execute_reply": "2025-06-14T16:08:59.855290Z",
     "shell.execute_reply.started": "2025-06-14T16:08:59.837389Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BlurDataset(Dataset):\n",
    "    def __init__(self, features_df: pd.DataFrame, scaler=None, fit_scaler=True):\n",
    "        super().__init__()\n",
    "        self.features_col = [col for col in features_df.columns if col != 'image_path' and col != 'label']\n",
    "        self.target_col = 'label'\n",
    "        \n",
    "        # Feature scaling\n",
    "        if scaler is None:\n",
    "            self.scaler = StandardScaler()\n",
    "            if fit_scaler:\n",
    "                features_scaled = self.scaler.fit_transform(features_df[self.features_col])\n",
    "            else:\n",
    "                features_scaled = features_df[self.features_col].values\n",
    "        else:\n",
    "            self.scaler = scaler\n",
    "            features_scaled = self.scaler.transform(features_df[self.features_col])\n",
    "        \n",
    "        self.X = torch.FloatTensor(features_scaled)\n",
    "        self.y = torch.FloatTensor(features_df[self.target_col].values).unsqueeze(1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4ddae-c64f-4a2c-acdd-adfd3e8c667d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:08:59.857547Z",
     "iopub.status.busy": "2025-06-14T16:08:59.857373Z",
     "iopub.status.idle": "2025-06-14T16:08:59.875257Z",
     "shell.execute_reply": "2025-06-14T16:08:59.874281Z",
     "shell.execute_reply.started": "2025-06-14T16:08:59.857534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_optimal_threshold(model, val_loader, device):\n",
    "    \"\"\"Find optimal threshold using validation set\"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_proba = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            y_true.extend(labels.numpy())\n",
    "            y_proba.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    # Different threshold\n",
    "    thresholds = np.arange(0.3, 0.8, 0.01)\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        preds = (np.array(y_proba) > threshold).astype(int)\n",
    "        from sklearn.metrics import f1_score\n",
    "        f1 = f1_score(y_true, preds)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    print(f\"Optimal threshold: {best_threshold:.3f} with F1: {best_f1:.4f}\")\n",
    "    return best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c057a-de71-415b-8426-2e588c8efdc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:00.138851Z",
     "iopub.status.busy": "2025-06-14T16:09:00.138465Z",
     "iopub.status.idle": "2025-06-14T16:09:00.144619Z",
     "shell.execute_reply": "2025-06-14T16:09:00.143779Z",
     "shell.execute_reply.started": "2025-06-14T16:09:00.138829Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_graph(train_loss_arr,val_loss_arr,val_acc_arr,best_val_loss,best_val_loss_epoch,total_epochs=100):\n",
    "    ep_arr=range(1,total_epochs+1)\n",
    "    plt.plot(ep_arr,train_loss_arr,color='r',label='Training Loss')\n",
    "    plt.plot(ep_arr,val_loss_arr,color='m',label='Validation Loss')\n",
    "    plt.plot(best_val_loss_epoch,best_val_loss,marker='x',color='g',label='Min Validation Loss',markersize=8)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('results/loss.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(ep_arr,val_acc_arr,color='b',label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('results/accuracy.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93810dca-b764-4a88-b640-9e6b5a3210b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:00.146293Z",
     "iopub.status.busy": "2025-06-14T16:09:00.146077Z",
     "iopub.status.idle": "2025-06-14T16:09:00.164873Z",
     "shell.execute_reply": "2025-06-14T16:09:00.164192Z",
     "shell.execute_reply.started": "2025-06-14T16:09:00.146278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_figs(y_true,y_pred,y_proba,class_names,save_path):    \n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "    pprint(report)\n",
    "    with open(f'{save_path}/classification_report.json','w') as f:\n",
    "        f.write(json.dumps(report))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm=confusion_matrix(y_true,y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d',cmap='Blues',xticklabels=class_names,yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_path}/cm.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    plt.plot(recall, precision, label=f'PR (AUC = {pr_auc:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{save_path}/pr_curve.png')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_path}/roc_plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae8519-f556-40d1-9c63-34cd1163f16d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:00.165895Z",
     "iopub.status.busy": "2025-06-14T16:09:00.165579Z",
     "iopub.status.idle": "2025-06-14T16:09:00.187975Z",
     "shell.execute_reply": "2025-06-14T16:09:00.187368Z",
     "shell.execute_reply.started": "2025-06-14T16:09:00.165870Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, epochs=150, lr=0.0005):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    class FocalLoss(nn.Module):\n",
    "        def __init__(self, alpha=1, gamma=2):\n",
    "            super().__init__()\n",
    "            self.alpha = alpha\n",
    "            self.gamma = gamma\n",
    "            \n",
    "        def forward(self, inputs, targets):\n",
    "            bce_loss = nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "            pt = torch.exp(-bce_loss)\n",
    "            focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "            return focal_loss.mean()\n",
    "    \n",
    "    criterion = FocalLoss(alpha=1, gamma=2) \n",
    "    \n",
    "    optimizer = AdamW([\n",
    "        {'params': model.input_layer.parameters(), 'lr': lr * 0.5},\n",
    "        {'params': model.block1.parameters(), 'lr': lr},\n",
    "        {'params': model.block2.parameters(), 'lr': lr},\n",
    "        {'params': model.block3.parameters(), 'lr': lr},\n",
    "        {'params': model.classifier.parameters(), 'lr': lr * 1.5}\n",
    "    ], weight_decay=1e-5)\n",
    "    \n",
    "    scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2)# Cosine annealing with warm restarts\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_val_loss_epoch = 0\n",
    "    train_loss_arr, val_loss_arr, val_accuracy_arr = [], [], []\n",
    "    patience_counter = 0\n",
    "    patience = 15\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_val_loss, epoch_val_correct = 0.0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                epoch_val_loss += criterion(outputs, labels).item()\n",
    "                preds = (outputs > 0.5).float()\n",
    "                epoch_val_correct += (preds == labels).sum().item()\n",
    "        \n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "        val_acc = epoch_val_correct / len(val_loader.dataset)\n",
    "        \n",
    "        train_loss_arr.append(avg_train_loss)\n",
    "        val_loss_arr.append(avg_val_loss)\n",
    "        val_accuracy_arr.append(val_acc)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping and model saving\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_val_loss_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'model/best_improved_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | '\n",
    "                  f'Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f} | LR: {current_lr:.2e}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(torch.load('model/best_improved_model.pth'))\n",
    "    return model, train_loss_arr, val_loss_arr, val_accuracy_arr,best_val_loss_epoch,best_val_loss,epoch+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6c52b-2b67-4b91-9335-84392f0644a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:00.188907Z",
     "iopub.status.busy": "2025-06-14T16:09:00.188693Z",
     "iopub.status.idle": "2025-06-14T16:09:00.207145Z",
     "shell.execute_reply": "2025-06-14T16:09:00.206598Z",
     "shell.execute_reply.started": "2025-06-14T16:09:00.188891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_datasets(features_df, train_test_val_split=[0.7, 0.15, 0.15]):\n",
    "    \"\"\"Create datasets with proper scaling\"\"\"\n",
    "    full_dataset_temp = BlurDataset(features_df, fit_scaler=True)\n",
    "    scaler = full_dataset_temp.scaler\n",
    "    \n",
    "    total_size = len(features_df)\n",
    "    train_size = int(train_test_val_split[0] * total_size)\n",
    "    test_size = int(train_test_val_split[1] * total_size)\n",
    "    val_size = total_size - train_size - test_size\n",
    "\n",
    "    indices = torch.randperm(total_size, generator=torch.Generator().manual_seed(69))\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:train_size + test_size]\n",
    "    val_indices = indices[train_size + test_size:]\n",
    "\n",
    "    train_df = features_df.iloc[train_indices].reset_index(drop=True)\n",
    "    test_df = features_df.iloc[test_indices].reset_index(drop=True)\n",
    "    val_df = features_df.iloc[val_indices].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = BlurDataset(train_df, scaler=scaler, fit_scaler=True)\n",
    "    test_dataset = BlurDataset(test_df, scaler=scaler, fit_scaler=False)\n",
    "    val_dataset = BlurDataset(val_df, scaler=scaler, fit_scaler=False)\n",
    "    \n",
    "    return train_dataset, test_dataset, val_dataset, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3da89d2-2bb3-47b4-872d-c77abe96b74f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:00.227669Z",
     "iopub.status.busy": "2025-06-14T16:09:00.227391Z",
     "iopub.status.idle": "2025-06-14T16:09:00.245283Z",
     "shell.execute_reply": "2025-06-14T16:09:00.244479Z",
     "shell.execute_reply.started": "2025-06-14T16:09:00.227628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model_with_threshold(model, test_loader, threshold=0.5, device=None, class_names=None):\n",
    "    \"\"\"Evaluate model with custom threshold and show confusion matrix\"\"\"\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    \n",
    "    if class_names is None:\n",
    "        class_names = [\"Box Blur\", \"Gaussian Blur\"]\n",
    "    \n",
    "    model.eval()\n",
    "    y_true, y_pred, y_proba = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            y_true.extend(labels.numpy())\n",
    "            y_pred.extend((outputs.cpu() > threshold).float().numpy())\n",
    "            y_proba.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_true, y_proba),\n",
    "        'threshold_used': threshold\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTest Metrics (threshold={threshold:.3f}):\")\n",
    "    for key, value in metrics.items():\n",
    "        if key != 'threshold_used':\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    generate_figs(y_true,y_pred,y_proba,[\"Box Blur\",\"Gaussian Blur\"],\"results\")\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b60670",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=DatasetGenerator(\"flickr30k_images/flickr30k_images\")\n",
    "features_df=obj.generate_dataset(output_path='featuresdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c3fd1-845f-48a3-bdff-b8cc65275c2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:00.469639Z",
     "iopub.status.busy": "2025-06-14T16:09:00.469322Z",
     "iopub.status.idle": "2025-06-14T16:09:00.521725Z",
     "shell.execute_reply": "2025-06-14T16:09:00.521130Z",
     "shell.execute_reply.started": "2025-06-14T16:09:00.469614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, val_dataset, scaler = create_datasets(features_df)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32 \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d9d91-7160-4d20-a366-6298b04fab20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:00.522709Z",
     "iopub.status.busy": "2025-06-14T16:09:00.522444Z",
     "iopub.status.idle": "2025-06-14T16:15:26.526296Z",
     "shell.execute_reply": "2025-06-14T16:15:26.525259Z",
     "shell.execute_reply.started": "2025-06-14T16:09:00.522686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "epochs=150\n",
    "model = BlurClassifierModel(input_features_no=10, dropout_rate=0.25)\n",
    "model, train_loss, val_loss, val_acc, best_val_loss_epoch, best_val_loss,last_epoch = train_model(\n",
    "    model, train_loader, val_loader, DEVICE, epochs=epochs, lr=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac4a510-6f4b-4546-8a3c-fdde62827a1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:15:56.888383Z",
     "iopub.status.busy": "2025-06-14T16:15:56.888116Z",
     "iopub.status.idle": "2025-06-14T16:15:57.558461Z",
     "shell.execute_reply": "2025-06-14T16:15:57.557700Z",
     "shell.execute_reply.started": "2025-06-14T16:15:56.888363Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_graph(train_loss,val_loss,val_acc,best_val_loss,best_val_loss_epoch,total_epochs=last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca80b3e-8d1a-4a40-bfbf-09288c8dec9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:16:42.827126Z",
     "iopub.status.busy": "2025-06-14T16:16:42.826420Z",
     "iopub.status.idle": "2025-06-14T16:16:44.910910Z",
     "shell.execute_reply": "2025-06-14T16:16:44.910128Z",
     "shell.execute_reply.started": "2025-06-14T16:16:42.827096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimal_threshold = find_optimal_threshold(model, val_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ce6f5-e861-4ba9-afb8-864a530becc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:16:48.468712Z",
     "iopub.status.busy": "2025-06-14T16:16:48.468386Z",
     "iopub.status.idle": "2025-06-14T16:16:50.501778Z",
     "shell.execute_reply": "2025-06-14T16:16:50.501110Z",
     "shell.execute_reply.started": "2025-06-14T16:16:48.468682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metrices=evaluate_model_with_threshold(model, test_loader, optimal_threshold)\n",
    "pprint(metrices)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7659951,
     "sourceId": 12162334,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
