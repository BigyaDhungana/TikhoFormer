{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a9dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from random import choice, shuffle\n",
    "import math\n",
    "import cv2\n",
    "import os\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# wandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\n",
    "# wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a475bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.__version__, torchvision.__version__, DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565dcbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR  = \"dataset/flickr30k_images\"\n",
    "IMG_DIMENSIONS = (128, 128)\n",
    "KERNEL_SIZES = (3, 5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84bdf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = os.listdir(BASE_DIR) [:100] # TODO: CHANGE\n",
    "all_images = [os.path.join(BASE_DIR, x) for x in all_images]\n",
    "len(all_images)\n",
    "\n",
    "test_image = cv2.imread(all_images[2])\n",
    "test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(test_image), test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59528f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, img_paths=all_images, train=True, test=False, blur_type=\"gaussian\",\n",
    "                 cache_size=50,\n",
    "                 train_ratio=0.8):\n",
    "        self.img_paths = img_paths\n",
    "        shuffle(self.img_paths)  # Shuffle once\n",
    "        self.dataset_length = len(self.img_paths)\n",
    "        self.blur = blur_type\n",
    "        \n",
    "        self.image_cache = {}\n",
    "        self.cache_size = min(cache_size, len(img_paths))\n",
    "        \n",
    "        # Pre-generate blur kernels\n",
    "        self._setup_blur_kernels()\n",
    "        \n",
    "        if train:\n",
    "            self.image_paths = self.img_paths[:math.floor(train_ratio*self.dataset_length)]\n",
    "        else:\n",
    "            self.image_paths = self.img_paths[math.ceil(train_ratio*self.dataset_length):]\n",
    "        \n",
    "        self._preload_images()\n",
    "        \n",
    "    \n",
    "    def _preload_images(self):\n",
    "        \"\"\"Cache frequently used images in memory\"\"\"\n",
    "        for i, path in enumerate(self.image_paths[:self.cache_size]):\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.resize(img, IMG_DIMENSIONS)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            self.image_cache[i] = transforms.ToTensor()(img)\n",
    "            \n",
    "    def _setup_blur_kernels(self):\n",
    "        \"\"\"Pre-create blur kernels as tensors\"\"\"\n",
    "        self.blur_kernels = {}\n",
    "        for ksize in KERNEL_SIZES:\n",
    "            if self.blur == \"gaussian\":\n",
    "                kernel = cv2.getGaussianKernel(ksize, 0)\n",
    "                kernel = np.outer(kernel, kernel.transpose())\n",
    "            else:  # box blur\n",
    "                kernel = np.ones((ksize, ksize)) / (ksize * ksize)\n",
    "            \n",
    "            self.blur_kernels[ksize] = torch.from_numpy(kernel).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Load image on demand\n",
    "        img_path = self.image_paths[index]\n",
    "        base_image = cv2.imread(img_path)\n",
    "        base_image = cv2.resize(base_image, IMG_DIMENSIONS)\n",
    "        \n",
    "        # Apply blur based on type\n",
    "        ksize = KERNEL_SIZES[index%3]\n",
    "        if self.blur == \"gaussian\":\n",
    "            blur_image = cv2.GaussianBlur(base_image, (ksize, ksize), 0)\n",
    "        elif self.blur == \"box\":\n",
    "            blur_image = cv2.blur(base_image, (ksize, ksize))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid blur type\")\n",
    "        \n",
    "        # Convert to RGB\n",
    "        base_image = cv2.cvtColor(base_image, cv2.COLOR_BGR2RGB)\n",
    "        blur_image = cv2.cvtColor(blur_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        base_tensor = transforms.ToTensor()(base_image)\n",
    "        blur_tensor = transforms.ToTensor()(blur_image)\n",
    "        return blur_tensor, base_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Flickr30kDataset(train=True, test=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, num_workers=2, pin_memory=True,persistent_workers=True)\n",
    "\n",
    "val_dataset = Flickr30kDataset(train=False, test=True)\n",
    "val_loader = DataLoader(train_dataset, batch_size=4, num_workers=2, pin_memory=True,persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c88af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================== IMPROVED DEBLURRING MODEL ======================\n",
    "class SharpDeblurViT(nn.Module):\n",
    "    def __init__(self, image_size=IMG_DIMENSIONS[0]):\n",
    "        super().__init__()\n",
    "        self.enc1 = self._make_encoder_block(3, 32, kernel_size=5, stride=1)\n",
    "        self.enc2 = self._make_encoder_block(32, 64, stride=2)\n",
    "        self.enc3 = self._make_encoder_block(64, 128, stride=2)\n",
    "        \n",
    "        # Transformer with multi-scale processing\n",
    "        self.bottleneck_size = image_size // 8\n",
    "        num_patches = self.bottleneck_size ** 2\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, 128))\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=128, \n",
    "                nhead=8, \n",
    "                dim_feedforward=256,\n",
    "                dropout=0.1, \n",
    "                activation='gelu',\n",
    "                norm_first=True,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        # Sharpness-enhancing decoder\n",
    "        self.dec1 = self._make_decoder_block(128, 64, scale_factor=2)\n",
    "        self.dec2 = self._make_decoder_block(64, 32, scale_factor=2)\n",
    "        \n",
    "        # Final reconstruction with residual connection\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(32 + 3, 32, kernel_size=3, padding=1),  # Input + skip\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(32, 3, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Edge enhancement module\n",
    "        self.edge_enhancer = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def _make_encoder_block(self, in_c, out_c, kernel_size=3, stride=2):\n",
    "        \"\"\"Use depthwise separable convolutions\"\"\"\n",
    "        return nn.Sequential(\n",
    "            # Depthwise convolution\n",
    "            nn.Conv2d(in_c, in_c, kernel_size, stride=stride, padding=kernel_size//2, groups=in_c),\n",
    "            # Pointwise convolution\n",
    "            nn.Conv2d(in_c, out_c, 1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout2d(0.1)\n",
    "        )\n",
    "    \n",
    "    def _make_decoder_block(self, in_c, out_c, scale_factor=2):\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.GELU(),\n",
    "            ResidualBlock(out_c, out_c)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial features\n",
    "        x0 = x  # Save input for residual connection\n",
    "    \n",
    "        # Encoder pathway\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        \n",
    "        # Transformer processing\n",
    "        B, C, H, W = e3.shape\n",
    "        patches = e3.flatten(2).transpose(1, 2)  # [B, N, C]\n",
    "        \n",
    "        # Adapt positional embedding to match the actual number of patches\n",
    "        pos_embed = self.pos_embed\n",
    "        if pos_embed.shape[1] != H * W:\n",
    "            # Resize positional embedding to match feature map size\n",
    "            pos_embed = nn.functional.interpolate(\n",
    "                pos_embed.reshape(1, int(math.sqrt(pos_embed.shape[1])), \n",
    "                                int(math.sqrt(pos_embed.shape[1])), C).permute(0, 3, 1, 2),\n",
    "                size=(H, W), mode='bilinear', align_corners=True\n",
    "            ).permute(0, 2, 3, 1).reshape(1, H*W, C)\n",
    "        \n",
    "        # Apply positional embedding\n",
    "        patches = patches + pos_embed\n",
    "        transformed = self.transformer(patches)\n",
    "        bottleneck = transformed.transpose(1, 2).view(B, C, H, W)\n",
    "        \n",
    "        # Decoder pathway\n",
    "        d1 = self.dec1(bottleneck)  # (B, 64, H/2, W/2)\n",
    "        d2 = self.dec2(d1)          # (B, 32, H, W)\n",
    "        \n",
    "        # Residual connection + final reconstruction\n",
    "        d2 = torch.cat([d2, x0], dim=1)  # Combine features with original input\n",
    "        output = self.final_conv(d2)\n",
    "        \n",
    "        # Edge refinement\n",
    "        edge = self.edge_enhancer(output)\n",
    "        return output + 0.3 * edge  # Sharpened output\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block for better gradient flow and feature preservation\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.activation(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity\n",
    "        return self.activation(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b590f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharpnessLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, beta=0.5, gamma=0.2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Pixel loss weight\n",
    "        self.beta = beta    # Frequency loss weight\n",
    "        self.gamma = gamma  # Edge loss weight\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "        sobel_x = torch.tensor([[[[1, 0, -1], [2, 0, -2], [1, 0, -1]]]], dtype=torch.float32)\n",
    "        sobel_y = torch.tensor([[[[1, 2, 1], [0, 0, 0], [-1, -2, -1]]]], dtype=torch.float32)\n",
    "        \n",
    "        self.register_buffer('sobel_x', sobel_x.repeat(3, 1, 1, 1))\n",
    "        self.register_buffer('sobel_y', sobel_y.repeat(3, 1, 1, 1))\n",
    "        \n",
    "        # Cache for FFT plans (PyTorch automatically optimizes repeated FFT sizes)\n",
    "        self.fft_cache_size = None\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Pixel loss (unchanged)\n",
    "        pixel_loss = F.l1_loss(pred, target) + 0.5 * F.mse_loss(pred, target)\n",
    "        \n",
    "        # Optimized frequency loss - compute less frequently\n",
    "        if self.training and torch.rand(1) < 0.3:  # Only 30% of the time during training\n",
    "            pred_fft = torch.fft.rfft2(pred, norm='ortho')\n",
    "            target_fft = torch.fft.rfft2(target, norm='ortho')\n",
    "            freq_loss = F.l1_loss(torch.abs(pred_fft), torch.abs(target_fft))\n",
    "        else:\n",
    "            freq_loss = 0.0\n",
    "        \n",
    "        # Optimized edge loss - use pre-registered kernels\n",
    "        edges_x_pred = F.conv2d(pred, self.sobel_x, padding=1, groups=3)\n",
    "        edges_y_pred = F.conv2d(pred, self.sobel_y, padding=1, groups=3)\n",
    "        edges_pred = torch.sqrt(edges_x_pred**2 + edges_y_pred**2 + 1e-6)\n",
    "        \n",
    "        edges_x_target = F.conv2d(target, self.sobel_x, padding=1, groups=3)\n",
    "        edges_y_target = F.conv2d(target, self.sobel_y, padding=1, groups=3)\n",
    "        edges_target = torch.sqrt(edges_x_target**2 + edges_y_target**2 + 1e-6)\n",
    "        \n",
    "        edge_loss = F.l1_loss(edges_pred, edges_target)\n",
    "        \n",
    "        return (self.alpha * pixel_loss + \n",
    "                self.beta * freq_loss + \n",
    "                self.gamma * edge_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86fee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeblurTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, config):\n",
    "        self.device = config['device']\n",
    "        self.model = model.to(self.device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        \n",
    "        if hasattr(torch, 'compile'):\n",
    "            self.model = torch.compile(self.model, mode='reduce-overhead')\n",
    "        \n",
    "        # Loss function with sharpness emphasis\n",
    "        self.loss_fn = SharpnessLoss(\n",
    "            alpha=1.0, \n",
    "            beta=0.7,  # Higher weight for frequency loss\n",
    "            gamma=0.3  # Edge preservation\n",
    "        )\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config['lr'],\n",
    "            weight_decay=config['weight_decay'],\n",
    "            fused=True  # Faster fused implementation\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=config['lr'],\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            epochs=config['epochs'],\n",
    "            pct_start=0.3\n",
    "        )\n",
    "        \n",
    "        # Mixed precision training\n",
    "        self.scaler = GradScaler()\n",
    "        \n",
    "        # Initialize wandb\n",
    "        # wandb.init(\n",
    "        #     project=config['project_name'],\n",
    "        #     config=config,\n",
    "        #     name=config['run_name'],\n",
    "        #     reinit=True\n",
    "        # )\n",
    "        # wandb.watch(model, log='all', log_freq=100)\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        grad_accum = self.config['grad_accum']\n",
    "        \n",
    "        for i, (blur, sharp) in tqdm(enumerate(self.train_loader), desc=\"Train Batch\"):\n",
    "            blur = blur.to(self.device, non_blocking=True)\n",
    "            sharp = sharp.to(self.device, non_blocking=True)\n",
    "            \n",
    "            # Mixed precision training\n",
    "            with autocast():\n",
    "                pred = self.model(blur)\n",
    "                loss = self.loss_fn(pred, sharp) / grad_accum\n",
    "            \n",
    "            # Backpropagation\n",
    "            self.scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient accumulation step\n",
    "            if (i + 1) % grad_accum == 0:\n",
    "                # Gradient clipping\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                \n",
    "                # Update weights\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # Update learning rate\n",
    "                self.scheduler.step()\n",
    "                \n",
    "                # Log batch loss\n",
    "                wandb.log({\n",
    "                    \"batch_loss\": loss.item() * grad_accum,\n",
    "                    \"lr\": self.scheduler.get_last_lr()[0]\n",
    "                })\n",
    "            \n",
    "            total_loss += loss.item() * grad_accum\n",
    "        \n",
    "        return total_loss / len(self.train_loader)\n",
    "    \n",
    "    def evaluate(self, epoch):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_psnr = 0.0\n",
    "        total_ssim = 0.0\n",
    "        count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for blur, sharp in tqdm(self.val_loader, desc=\"Val Batch\"):\n",
    "                blur = blur.to(self.device)\n",
    "                sharp = sharp.to(self.device)\n",
    "                \n",
    "                pred = self.model(blur)\n",
    "                pred = torch.clamp(pred, 0, 1)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = self.loss_fn(pred, sharp)\n",
    "                total_loss += loss.item() * blur.size(0)\n",
    "                \n",
    "                # Calculate PSNR\n",
    "                mse = torch.mean((pred - sharp) ** 2)\n",
    "                psnr_val = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
    "                total_psnr += psnr_val.item() * blur.size(0)\n",
    "                \n",
    "                # Calculate SSIM\n",
    "                mu_x = pred.mean(dim=[1, 2, 3])\n",
    "                mu_y = sharp.mean(dim=[1, 2, 3])\n",
    "                sigma_x = pred.var(dim=[1, 2, 3])\n",
    "                sigma_y = sharp.var(dim=[1, 2, 3])\n",
    "                sigma_xy = torch.mean(pred * sharp, dim=[1, 2, 3]) - mu_x * mu_y\n",
    "                \n",
    "                ssim_val = ((2 * mu_x * mu_y) * (2 * sigma_xy)) / \\\n",
    "                          ((mu_x**2 + mu_y**2) * (sigma_x + sigma_y) + 1e-8)\n",
    "                total_ssim += ssim_val.sum().item()\n",
    "                count += blur.size(0)\n",
    "                \n",
    "                # Log sample images every 3 epochs\n",
    "                if epoch % 3 == 0 and count < 16:  # Log first few batches\n",
    "                    self.log_sample_images(blur, sharp, pred)\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss / count,\n",
    "            'PSNR': total_psnr / count,\n",
    "            'SSIM': total_ssim / count\n",
    "        }\n",
    "    \n",
    "    def log_sample_images(self, blur, sharp, pred):\n",
    "        \"\"\"Log comparison images to wandb\"\"\"\n",
    "        # Convert to numpy and denormalize if needed\n",
    "        blur_np = blur.cpu().numpy()\n",
    "        sharp_np = sharp.cpu().numpy()\n",
    "        pred_np = pred.cpu().numpy()\n",
    "        \n",
    "        # Log first 3 samples\n",
    "        images = []\n",
    "        for i in range(min(3, blur.size(0))):\n",
    "            # Calculate sharpness metrics\n",
    "            sharpness_gt = self.calculate_sharpness(sharp[i])\n",
    "            sharpness_pred = self.calculate_sharpness(pred[i])\n",
    "            \n",
    "            images.append(wandb.Image(\n",
    "                blur_np[i].transpose(1, 2, 0), \n",
    "                caption=f\"Blurred Input (Epoch {self.current_epoch})\"\n",
    "            ))\n",
    "            images.append(wandb.Image(\n",
    "                sharp_np[i].transpose(1, 2, 0), \n",
    "                caption=f\"Sharp Target | Sharpness: {sharpness_gt:.3f}\"\n",
    "            ))\n",
    "            images.append(wandb.Image(\n",
    "                pred_np[i].transpose(1, 2, 0), \n",
    "                caption=f\"Predicted | Sharpness: {sharpness_pred:.3f}\"\n",
    "            ))\n",
    "        \n",
    "        wandb.log({\"results\": images})\n",
    "    \n",
    "    def calculate_sharpness(self, image_tensor):\n",
    "        \"\"\"Calculate sharpness metric (variance of Laplacian)\"\"\"\n",
    "        # Convert to grayscale\n",
    "        if image_tensor.shape[0] == 3:  # RGB\n",
    "            gray = 0.2989 * image_tensor[0] + 0.5870 * image_tensor[1] + 0.1140 * image_tensor[2]\n",
    "        else:\n",
    "            gray = image_tensor[0]\n",
    "        \n",
    "        # Create proper 4D tensor for input (B,C,H,W)\n",
    "        gray_input = gray.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Create proper 4D Laplacian kernel (out_channels, in_channels, H, W)\n",
    "        # and move to the same device as the input\n",
    "        laplacian_kernel = torch.tensor([[[[0, 1, 0], \n",
    "                                        [1, -4, 1], \n",
    "                                        [0, 1, 0]]]], \n",
    "                                        dtype=torch.float32, \n",
    "                                        device=image_tensor.device)\n",
    "        \n",
    "        # Apply convolution\n",
    "        laplacian = F.conv2d(gray_input, laplacian_kernel, padding=1)\n",
    "        \n",
    "        return torch.var(laplacian).item()\n",
    "    \n",
    "    def fit(self):\n",
    "        best_psnr = 0\n",
    "        best_sharpness = 0\n",
    "        early_stop_counter = 0\n",
    "        patience = 8\n",
    "        \n",
    "        for epoch in tqdm(range(self.config['epochs']), desc=\"Epoch\"):\n",
    "            self.current_epoch = epoch\n",
    "            \n",
    "            # Train for one epoch\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            \n",
    "            # Validate\n",
    "            val_metrics = self.evaluate(epoch)\n",
    "            \n",
    "            # Log metrics\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_metrics['loss'],\n",
    "                \"val_psnr\": val_metrics['PSNR'],\n",
    "                \"val_ssim\": val_metrics['SSIM']\n",
    "            })\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{self.config['epochs']} | \"\n",
    "                  f\"Train Loss: {train_loss:.5f} | \"\n",
    "                  f\"Val Loss: {val_metrics['loss']:.5f} | \"\n",
    "                  f\"PSNR: {val_metrics['PSNR']:.2f} dB | \"\n",
    "                  f\"SSIM: {val_metrics['SSIM']:.4f}\")\n",
    "            \n",
    "            # Early stopping and model saving\n",
    "            if val_metrics['PSNR'] > best_psnr:\n",
    "                best_psnr = val_metrics['PSNR']\n",
    "                torch.save(self.model.state_dict(), \"best_model.pth\")\n",
    "                wandb.save(\"best_model.pth\")\n",
    "                print(f\"Saved best model with PSNR: {best_psnr:.2f} dB\")\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        # Save final model\n",
    "        torch.save(self.model.state_dict(), \"final_model.pth\")\n",
    "        wandb.save(\"final_model.pth\")\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf0d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'project_name': 'image-deblurring-sharp',\n",
    "    'run_name': 'sharp-vit-128',\n",
    "    'image_size': IMG_DIMENSIONS[0],\n",
    "    'batch_size': 8,\n",
    "    'grad_accum': 4,\n",
    "    'epochs': 50,\n",
    "    'lr': 3e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec7cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = SharpDeblurViT(image_size=config['image_size'])\n",
    "\n",
    "# Calculate parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "config['parameters'] = f\"{total_params/1e6:.2f}M\"\n",
    "print(f\"Model parameters: {config['parameters']}\")\n",
    "\n",
    "# Create datasets and loaders (replace with your data)\n",
    "\n",
    "transform = Compose([\n",
    "    Resize((config['image_size'], config['image_size'])),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# Train model\n",
    "trainer = DeblurTrainer(model, train_loader, val_loader, config)\n",
    "# trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
